# Practice Problems 6

## Problem 1

### Beer Example

A study of 16 Ohio State University students looked at the relationship between the number of beers a student consumes and their blood alcohol content (BAC) 30 minutes after their last beer.  The regression information from R to predict BAC from number of beers consumed is given below. 


```{r, fig.width=4, fig.height=4}
library(ggplot2); library(dplyr)
bac <- read.csv("https://raw.githubusercontent.com/deepbas/statdatasets/main/BAC.csv")
ggplot(data = bac, aes(x = Beers, y = BAC)) + geom_point(shape = 19) + labs(title = "Beer and BAC", x = "Number of beers", y = "Blood Alcohol Content") + theme_minimal()
```


- Is there a relationship?
    + direction?
    + strength?
    + form?


### (a) Computing correlation

Compute correlation and account for missing values.

```{r}
cor(bac$BAC, bac$Beers, use = "complete.obs")
```

`Note`: If there are any missing values (`NA`'s) on either of the variables involved in the correlation calculation, use `use = "complete.obs"` as an extra argument to the `cor` function.


### (b)	Fitting a regression line

We use the `lm(y ~ x, data=mydata)` function to fit a **l**inear (regression) **m**odel for a response y given an explanatory variable x. This command creates a **linear model object** that needs to be assigned a name, here we call it `bac.lm`. You can get the slope and intercept by typing out the object name:

```{r}
bac.lm <- lm(BAC ~ Beers, data=bac)
bac.lm
```

### (c) Write down the fitted regression equation to predict BAC from number of beers. 

<details>

<summary><red>Click for answer</red></summary>
*Answer:* $\hat{y} = -0.01270 + 0.01796*x$
</details><br>



- You can add this regression line to your scatterplot from part (a) by creating the plot and using the `abline` command: 


```{r}
ggplot(data = bac, aes(x = Beers, y = BAC)) + 
  geom_point(shape = 19) + 
  geom_smooth(method = "lm", se = FALSE, color = "blue") + 
  labs(title = "Beer and BAC", x = "Number of beers", y = "Blood Alcohol Content") + 
  theme_minimal()
```



### (d)	Interpret the slope in context.

<details>
<summary><red>Click for answer</red></summary>
*Answer:* Drinking one more beer is associated with a 0.0180 unit increase in predicted BAC.
</details><br>

### (e)	Interpret the intercept in context, if it makes sense to do so. 

<details>
<summary><red>Click for answer</red></summary>
*Answer:* The intercept is -0.0127.  A student who drinks 0 beers would be predicted to have a negative blood alcohol content. This is not possible so the intercept does not make sense in this context, but the intercept is included in the model to get the best fit line for the data collected.
</details><br>


### (f)	If your friend at Ohio State drank 2 beers, what would you predict their BAC to be?

<details>
<summary><red>Click for answer</red></summary>
*Answer:* The predicted BAC is 0.0233.

$$
\widehat{BAC} = -0.0127 + 0.0180(2) = 0.0233.
$$

```{r}
BAC.hat <- -0.0127 + 0.0180*(2) 
BAC.hat
```

</details><br>

### (g)	Find the residual for the student in the dataset who drank 2 beers and had a BAC of 0.03.

<details>
<summary><red>Click for answer</red></summary>
*Answer:* The residual is 0.0067.

$$
BAC - \widehat{BAC} = .03 - .0233=0.0067
$$

```{r}
0.03 - (-0.0127 + 0.0180*(2)) 
```

</details><br>

### (h) Getting residuals in R

We can use the `resid` command to get the residuals for each case in the data set:

```{r}
# Residuals 
residuals <- data.frame(Beers = bac$Beers, Residuals = resid(bac.lm))
residuals
```



### (i) Getting $R^2$ value

You can use the `summary` command on an `lm` object to get a more detailed print out of your linear model, along with the $R^2$ value for your model:

```{r}
summary(bac.lm)
```


### (j) Making a residuals plot

The regression of `BAC` on `Beers` has a residuals plot that plots the model's residuals on the y-axis and the explanatory ("predictor") on the x-axis. We add a horizontal reference line (the detrended regression line) with the `geom_hline()` command:

```{r, fig.width=4, fig.height=4}
# code for residual plot
ggplot(data = residuals, aes(x = Beers, y = Residuals)) +
           geom_point(shape = 19) +
           geom_hline(yintercept = 0, color = "blue") +
           labs(title = "Residuals Plot",
                x = "Number of beers drank",
                y = "Residuals") +
           theme_minimal()
```

**Interpret:** There is one case of 9 beers with a large residual (much higher BAC than predicted), but since there is no clear pattern (trend) in this plot it looks like our regression model adequately describes the relationship between number of beers and BAC. 

- Is the magnitude of the scatter around the horizontal 0-line in the residuals plot greater than, less than, or the same as the magnitude of the scatter around the regression line in the scatterplot?

<details>
<summary><red>Click for answer</red></summary>
*Answer:* The same! The residuals plot is only a "detrended" scatterplot, meaning the vertical distances between a point and the regression line on the scatterplot or a point and the 0-line on the residuals plot are exactly the same. The residual plot looks more scattered because the trend is removed and the scale of the y-axis compressed. 
</details><br>

### (k) Identifying points

To find rows where the number of Beers is 9, we can use the `filter` function from the `dplyr` package.

```{r}
# Use `filter` to find rows where Beers equals 9
filtered_data <- filter(bac, Beers == 9)
filtered_data
```


<details>
<summary><red>Click for answer</red></summary>
*Answer:* Row 3. 
</details><br>

What is the row number of the case with the most negative residual? We could eyeball the graph to see that the most negative residual is less than -0.02:

To find the row number of the case with the most negative residual, you can identify residuals less than -0.02, and then pinpoint the one that also drank 5 beers:


```{r, fig.width=4, fig.height=4}
# Use `filter` to identify rows with residuals less than -0.02 and 5 beers
filtered_resid <- filter(bac, resid(bac.lm) < -0.02 & Beers == 5)
filtered_resid
```

### (l) Checking outlier influence


You can examine the influence of removing specific cases on your model by using the `subset` parameter in the `lm` function. For instance, to remove row 3:

```{r}
# Fit a linear model without row 3
bac.lm2 <- lm(BAC ~ Beers, data = bac, subset = -3)
```

```{r}
# Compare the two models
summary(bac.lm2)
summary(bac.lm)
```

- After removing case 3, how has the slope changed? Explain the why the change occurred.

<details>
<summary><red>Click for answer</red></summary>
*Answer:* The slope drops from 0.0180 to 0.0146. Explanation given above.
</details><br>


- After removing case 3, how has the $R^2$ changed? Explain the why the change occurred.

<details>
<summary><red>Click for answer</red></summary>
*Answer:* The $R^2$ descreases from 79.9% to 76.8%. This small decrease happens because case 3 actually enhances the overall linear trend and removing it results is a slight decrease to correlation and $R^2$. 
</details><br>

### (m) Adding a categorical variable to your plot

To color-code points by a categorical variable like `Gender`, you can use the `ggplot2` package as follows:

```{r}
ggplot(bac, aes(x = Beers, y = BAC, color = Gender)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)
```

- Are the associations similar? (form, strength, direction)

<details>
<summary><red>Click for answer</red></summary>
*Answer:* Both females and males have similar strong, positive linear associations.
</details><br>

### (n) Regression lines by groups

To investigate the relationship of `Beers` and `BAC` for different genders, we can fit separate linear models.

#### For Females:

```{r}
# Use `filter` to get female data
bac_female <- filter(bac, Gender == "female")

# Fit the model for females
bac_lm_female <- lm(BAC ~ Beers, data = bac_female)
summary(bac_lm_female)
```


#### For Males:


```{r}
# Use `filter` to get male data
bac_male <- filter(bac, Gender == "male")

# Fit the model for males
bac_lm_male <- lm(BAC ~ Beers, data = bac_male)
summary(bac_lm_male)
```


- What is the regression line for females? for males?

<details>
<summary><red>Click for answer</red></summary>
*Answer:* For females: $\widehat{BAC} = -0.016 +0.021(BAC)$ and for males:  $\widehat{BAC} = -0.01 +0.015(BAC)$
</details><br>

- Which gender has the largest slope? What does this suggest about the relationship between number of beers and BAC for this gender?

<details>
<summary><red>Click for answer</red></summary>
*Answer:* The slope for females is slightly higher. This shows that the effect of one more beer on predicted BAC in females is larger than males (a 0.021 increase vs. a 0.015 increase).


---



## Problem 2: Inkjet Example

The following `Inkjet` dataset deals with 20 observations on the following 6 variables:

- `Model` Model name of printer
- `PPM` Printing rate (pages per minute) for a benchmark set of print jobs
- `PhotoTime` Time (in seconds) to print $4 \times 6$ color photos
- `Price` Typical retail price (in dollars)
- `CostBW` Cost per page (in cents) for printing in black \& white
- `CostColor` Cost per page (in cents) for printing in color

```{r, echo=FALSE}
library(ggplot2)
library(dplyr)
inkjet <- read.csv("https://www.lock5stat.com/datasets2e/InkjetPrinters.csv") 
knitr::kable(inkjet)
```


```{r, fig.width=5, fig.height=3, echo=FALSE}
ggplot(data = inkjet, aes(x = Price, y = CostColor)) + 
     geom_point(shape = 19) +
     labs(title = "Price and Cost to print a page in Color",
          x = "Price (in dollars)",
          y = "Cost per page (in cents)") +
     theme_minimal()
```

### (a) Is there a relationship?
    + direction?
    + strength?
    + form?

<!-- <details> -->
<!-- <summary><red>Click for answer</red></summary> -->
<!-- *Answer:* There is a negative, moderate, and linear relationship. -->
<!-- </details><br> -->


We use the `lm(y ~ x, data=mydata)` function to fit a **l**inear (regression) **m**odel for a response y given an explanatory variable x. 

```{r}
inkjet.lm <- lm(CostColor ~ Price, data=inkjet)
summary(inkjet.lm)
```


### (b) Write down the fitted regression equation to predict cost per page based on the price. 


<!-- <details> -->
<!-- <summary><red>Click for answer</red></summary> -->
<!-- *Answer:* $\widehat{CostColor} = 15.3514 - 0.02278 * \text{Price}$ -->

<!-- </details><br> -->

### (c)	Interpret the slope in context.

<!-- <details> -->
<!-- <summary><red>Click for answer</red></summary> -->
<!-- *Answer:* For each dollar increase in the price of the inkjet printer, there is a decrease in the cost to print in color by 0.02278 cents. -->

<!-- </details><br> -->

### (d)	Interpret the intercept in context, if it makes sense to do so. 

<!-- <details> -->
<!-- <summary><red>Click for answer</red></summary> -->
<!-- *Answer:* For a price of printer that costs 0 dollars, the cost to print in color is 15.3514 cents. It is an extrapolation and it's nonsensical to talk about a printer costing 0 dollars in this context. -->

<!-- </details><br> -->

### (f)	If you buy an injet printers worth 200 dollars, what would you estimate the cost per page (in cents)?

<!-- <details> -->
<!-- <summary><red>Click for answer</red></summary> -->
<!-- *Answer:* The cost per page would be 10.80 cents. -->

<!-- ```{r} -->
<!-- 15.3514 - 0.02278*200 -->
<!-- ``` -->

<!-- </details><br> -->

### (g) What is the value of $R^2$, the coefficient of determination? What does it mean?

<!-- <details> -->
<!-- <summary><red>Click for answer</red></summary> -->
<!-- *Answer:* The value of $R^2$ is 0.4228. It means that about 42.28% of variability in the cost to print in color is explained by the price of the inkjet printer.  -->

<!-- </details><br> -->

### (h) What is the value of $r$, the correlation coefficient?

<!-- <details> -->
<!-- <summary><red>Click for answer</red></summary> -->
<!-- *Answer:* The value of $r$ is - 0.65. -->

<!-- ```{r} -->
<!-- sqrt(0.4228) -->
<!-- ``` -->



<!-- Note: It's important to note that $R^2$ and $r$ are related but not directly convertible just by taking a square root, especially in multiple regression scenarios. The square root of $R^2$ only equals $r$ in simple linear regression models, and even then, the sign of $r$ depends on the sign of the slope. -->

<!-- </details><br> -->
